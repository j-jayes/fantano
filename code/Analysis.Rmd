---
title: "Analysis"
author: "JJayes"
date: "01/07/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(lubridate)
library(glue)
library(scales)

theme_set(theme_light())
```

## Purpose

EDA and modelling for Needle Drop Dataset to predict score from words

### Reading in data

```{r}
df <- read_rds("data/nice_nice_nice_data.rds")

colnames(df)

# we have some missing score data so we will filter it out.
df <- df %>% 
  filter(!is.na(score))

df <- df %>%
  group_by(video_id) %>% 
  mutate(across(track_explicit:time_signature, median)) %>% 
  ungroup() %>% 
  distinct(video_id, .keep_all = T)

```

## EDA

What does the data look like?

```{r}
df %>% skimr::skim()
```

### How does Fantano score albums?

```{r}
df %>% 
  count(score) %>% 
  ggplot(aes(score, n)) +
  geom_col(fill = "lightblue") + # Come back to colours
  scale_x_continuous(labels = number_format(accuracy = 1)) +
  labs(x = "Album score by Fantano",
       y = "Number of albums")
```

A lovely distribution centred around 7/10.

### Scores over time?

```{r}
library(plotly)

g <- df %>% 
  mutate(title = str_remove(video_title, "ALBUM REVIEW")) %>% 
    ggplot(aes(published_date, score, text = glue("{title} - {score}"))) +
    geom_jitter(aes(colour = score),
                width = 0, height = .6, show.legend = FALSE) +
    geom_smooth(method = "lm") +
    labs(y = "Album score",
         x = "Review date")

ggplotly(g, tooltip = "text")

```

With ggiraph

```{r}
library(ggiraph)

glue("{spotify_album_name} - Score: {score}")

scatter <- df_mod %>% 
  mutate(onclick = glue('window.open("{url}")'),
         caption = spotify_album_name) %>% 
  ggplot(aes(published_date, score)) +
  geom_point_interactive(aes(tooltip = caption,
                             data_id = caption,
                             onclick = onclick)) +
  labs(x = "Date of review",
       y = "Score")


int_fig <- girafe(
  ggobj = scatter,
  width_svg = 5,
  height_svg = 3,
  options = list(
    # opts_tooltip(css = tooltip_css, delay_mouseover = 0, delay_mouseout = 0),
    opts_hover(css = "colour: #E69F00; fill-opacity: 1.0; stroke: #E69F00;")
  )
)

int_fig

df %>% 
  mutate(title = str_remove(video_title, "ALBUM REVIEW"),
         text_ = glue("{title} - {score}")) %>% 
    ggplot(aes(published_date, score)) +
    geom_point_interactive(aes(tooltip = text_, 
                                data_id = text_),
                na.rm = TRUE) +
    geom_smooth(method = "lm") +
    labs(y = "Album score",
         x = "Review date")



```


### Scores and video statistics?

```{r}
df %>% 
  select(score, contains("statistics")) %>% 
  pivot_longer(-score, names_to = "stat") %>% 
  mutate(stat = str_replace_all(stat, "_", " "),
         stat = str_to_title(stat)) %>% 
  ggplot(aes(value, score)) +
  geom_jitter(alpha = .5,
              height = .8,
              colour = "lightblue") +
  geom_smooth() +
  scale_x_log10(labels = scales::number_format(accuracy = 1)) +
  facet_wrap(~ stat, scales = "free_x") +
  labs(x = NULL,
       y = NULL,
       title = "How do viewers of Needle Drop videos respond to different scores?")

```

What can we say about this?

As people comment more and dislike the video, the scores get lower, potentially indicative of anger at Fantano scoring the albums lower than his viewers would have liked.

We can also make a linear model and display the results as a tie-fighter plot. This shows that when we log the statistic, comment count and dislike count are negatively associated with the score.

```{r}
df %>% 
  select(score, contains("statistics")) %>% 
  pivot_longer(-score, names_to = "stat") %>% 
  mutate(stat = str_replace_all(stat, "_", " "),
         stat = str_to_title(stat),
         value = log(value)) %>% 
  nest(data = -stat) %>% 
  mutate(
    fit = map(data, ~ lm(score ~ value, data = .x)),
    tidied = map(fit, ~ tidy(.x, conf.int = T))
  ) %>% 
  unnest(tidied) %>% 
  filter(term != "(Intercept)") %>% 
  ggplot(aes(estimate, stat)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(colour = "lightblue", cex = 5) +
  geom_vline(xintercept = 0, lty = 2) +
  labs(x = "Effect of log of term on album score",
       y = NULL,
       title = "What is the correlation between view statistics and album score?")

```

What is the relationship between duration of video and the score? Does Fantano spend more time time describing an album that he really enjoyed?

```{r}
# df %>% 
#   ggplot(aes(duration, score)) +
#   geom_smooth() +
#   geom_jitter(colour = "lightblue") +
#   scale_x_log10(lim = c(150, 3000)) +
#   labs(x = "Duration of video in seconds",
#        y = "Album score") +
#   scale_y_continuous(labels = scales::number_format(accuracy = 1)) 

df %>% 
  lm(score ~ log(duration), data = .) %>% tidy() 
  # kableExtra::kable() maybe do this for post
```

How else can we ask this question?

```{r}
library(ggridges)

df %>% 
  # filter(score > 2) %>% maybe include this limit?
  ggplot(aes(x = duration, y = factor(score), fill = score)) +
  geom_density_ridges(show.legend = F) +
  scale_fill_gradient2(low = "blue", high = "orange", midpoint = 5) +
  scale_x_log10(lim = c(200, 2000)) +
  labs(y = "Album score",
       x = "Duration of video in seconds",
       title = "How does review duration relate to score?")
```

### Text

What does he say in his reviews?

```{r}
library(tidytext)

words <- df %>% 
  unnest_tokens(word, text) %>% 
  select(score, video_title, word)

words <- words %>% 
  anti_join(stop_words)

words %>% 
  count(word, sort = T) %>% 
  head(100) %>% view()

words %>% 
  count(word, sort = T) %>% 
  slice_max(n, n = 50, with_ties = F) %>% 
  mutate(tags = fct_reorder(tags, n)) %>% 
  ggplot(aes(n, tags, fill = n)) +
  geom_col()

```


### Tags

```{r}
tags <- df
  
tags <- tags %>% select(tags) %>% 
  unnest_tokens(output = tags, input = tags, token = stringr::str_split, pattern = ", ") %>% 
  anti_join(stop_words, by = c("tags" = "word"))

tags %>% 
  count(tags, sort = T) %>% 
  slice_max(n, n = 50, with_ties = F) %>% 
  mutate(tags = fct_reorder(tags, n)) %>% 
  ggplot(aes(n, tags, fill = n)) +
  geom_col()

tags %>% 
  filter(str_detect(tags, "rvwz")) %>% 
  count(tags, sort = T)

df %>% 
  filter(str_detect(tags, "rvwz"))

# so we can remove the rvwz from the tags
```

### Audio features?

```{r}
df <- read_rds("data/audio_features_small.rds")

test_7 <- df %>% nest(data = c(track_name, track_id, track_explicit, danceability, energy, 
    key, loudness, mode, speechiness, acousticness, instrumentalness, 
    liveness, valence, tempo, duration_ms, time_signature, spotify_track_features)) %>% 
  distinct(video_id, .keep_all = T)



df %>% 
  ggplot(aes(danceability)) +
  geom_density()

df %>% 
  ggplot(aes(danceability, instrumentalness, colour = factor(score))) +
  geom_point()

df %>% 
  count(time_signature, sort = T)

df %>% count(track_explicit, sort = T)

df %>% count(key, time_signature, sort = T)
```


```{r}
df %>% 
  ggplot(aes(spotify_artist_popularity, score)) +
  geom_jitter() +
  geom_smooth(method = "lm")

```

Hahah he's such a hipster. more popular leading to lower scores.

```{r}
df %>% 
  ggplot(aes(x = spotify_artist_popularity, y = factor(score), fill = score)) +
  geom_density_ridges(show.legend = F) +
  scale_fill_gradient2(low = "blue", high = "orange", midpoint = 5) +
  labs(y = "Album score",
       x = "spotify_artist_popularity",
       title = "How does spotify_artist_popularity relate to score?")

```

Are the fans normies?? 

How can we tell? We need to correct for popularity of artist when analysing how much they agree or disagree with Fantano. Maybe divide comments and dislikes by views?

Becasuse it makes sense that comment count increases with populairty - you'll watch the reviews that are of artists you care about.

```{r}
df %>% 
  ggplot(aes(video_statistics_comment_count, spotify_artist_popularity)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10()

```

That's hilarious!

If we correct for views it's not as strong.

```{r}
df %>% 
  mutate(comments_over_views = video_statistics_comment_count / video_statistics_view_count) %>% 
  ggplot(aes(comments_over_views, spotify_artist_popularity)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10()
```

What about the ratio of likes to dislikes? Or comments to dislikes?

```{r}
df %>% 
  mutate(likes_to_dislikes = video_statistics_like_count / video_statistics_dislike_count) %>% 
  ggplot(aes(likes_to_dislikes)) +
  geom_density() +
  facet_wrap( ~ score, scales = "free_y")

```



Another way to put this is to look at the popularity of the artist and the dislike count. wait what about ratio or views to dislikes??

```{r}
df %>% 
  ggplot(aes(video_statistics_dislike_count, spotify_artist_followers)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  scale_y_log10()

```


```{r}
df %>% 
  mutate(dislikes_over_views = video_statistics_dislike_count / video_statistics_view_count) %>% 
  ggplot(aes(dislikes_over_views, spotify_artist_followers)) +
  geom_point() +
  geom_smooth(method = "lm") +
  scale_x_log10() +
  scale_y_log10()
```

We could do a regression with dislikes as dep var and popularity and views as xs?

```{r}
df %>% 
  lm(log(video_statistics_dislike_count) ~ log(spotify_artist_followers) + 
       log(video_statistics_view_count), 
     data = .) %>% tidy()
```

```{r}
df %>% 
  ggplot(aes(x = danceability, y = factor(score), fill = score)) +
  geom_density_ridges(show.legend = F) +
  scale_fill_gradient2(low = "blue", high = "orange", midpoint = 5) +
  labs(y = "Album score",
       x = "danceability",
       title = "How does danceability relate to score?")

df %>% 
  group_by(video_id) %>% 
  mutate(danceability_mean = mean(danceability)) %>% 
  ungroup() %>% 
  ggplot(aes(x = danceability_mean, y = factor(score), fill = score)) +
  geom_density_ridges(show.legend = F) +
  scale_fill_gradient2(low = "blue", high = "orange", midpoint = 5) +
  labs(y = "Album score",
       x = "danceability",
       title = "How does danceability relate to score?")

df %>% 
  group_by(video_id) %>% 
  mutate(danceability_var = var(danceability)) %>% 
  ungroup() %>% 
  ggplot(aes(x = danceability_var, y = factor(score), fill = score)) +
  geom_density_ridges(show.legend = F) +
  scale_fill_gradient2(low = "blue", high = "orange", midpoint = 5) +
  labs(y = "Album score",
       x = "danceability",
       title = "How does danceability relate to score?")
```

Popularity?

```{r}
df %>% 
  mutate(spotify_artist_popularity = 
           spotify_artist_popularity - spotify_artist_popularity %% 10) %>% 
  ggplot(aes(x = danceability, y = factor(spotify_artist_popularity), fill = spotify_artist_popularity)) +
  geom_density_ridges(show.legend = F) +
  scale_fill_gradient2(low = "blue", high = "orange", midpoint = 50) +
  labs(y = "spotify_artist_popularity",
       x = "danceability",
       title = "How does danceability relate to spotify_artist_popularity?")

```


```{r}
df_features <- df %>% 
  select(score, spotify_artist_popularity, danceability:time_signature) %>% 
  rename_all(.funs = ~ str_replace_all(.x, "_", " ")) %>%
  rename_all(.funs = str_to_title) %>% 
  pivot_longer(-c(Score, `Spotify Artist Popularity`), 
               names_to = "audio_feature_name", 
               values_to = "audio_feature_value") %>% 
  pivot_longer(-c(audio_feature_name, audio_feature_value), 
               names_to = "music_metric", 
               values_to = "score_value")



draw_features_ridges <- function(tbl, feature, metric){
  
  tbl <- tbl %>% 
    filter(audio_feature_name == feature,
           music_metric == metric) %>% 
    mutate(score_value = if_else(music_metric == "Spotify Artist Popularity", 
                                  score_value - score_value %% 10, 
                                  score_value))
  
  mid_point = mean(tbl$score_value)
  
  mean_feature_value = round(mean(tbl$audio_feature_value, na.rm = T), 2)
  
  tbl %>% 
    ggplot(aes(audio_feature_value, factor(score_value), fill = score_value)) +
    geom_density_ridges(show.legend = F) +
    scale_fill_gradient2(low = "blue", high = "orange", midpoint = mid_point) +
    geom_vline(xintercept = mean_feature_value, lty = 2) + 
    labs(y = glue("{metric}"),
         x = glue("{feature}"),
         title = glue("How does {feature} relate to {metric}?"),
         subtitle = glue("Mean {feature} is {mean_feature_value}"))
  
}

draw_features_ridges(df_features, "Energy", "Spotify Artist Popularity")

draw_features_ridges(df_features, "Energy", "Score")

```

Crosstalk for a plot that allows selection without shiny

```{r}
library(crosstalk)

shared_features <- df_features


bscols(widths = c(3,NA),
  list(
    filter_select("audio_feature_name", "audio_feature_name", shared_features,
                  ~ifelse(audio_feature_name == 0, "Danceability", "Energy", "Loudness")),
  ),
  d3scatter(shared_features, ~audio_feature_value, ~score_value, ~factor(score_value), width="100%", height=250),
)


```

### Recipe for modelling

Okay I think this is actually a two part problem. In the first part we want to find the words that are correlated with score, in the second part we actually want to do prediction - using a tree-based model. These don't work well with a lot of features that are sparse. Can maybe introduce a main genre in this second part to make use of genres?

In the text analysis we want to do 3 things.

First we want to see what genres he scores highly or poorly.

Next we want to look at the words, bigrams and maybe trigrams that are correlated with a high or low score.

Third we can try some LDA to see what he has spoken about. have a look at [step lda in textrecipes package](https://textrecipes.tidymodels.org/reference/step_lda.html)

Prep for modelling

```{r}
df <- read_rds("data/nice_nice_nice_data_compress.rds")

df %>% 
  skimr::skim()

df %>% 
  tail() %>% view()
```

We stopped getting track features at some point.

So for now we will just filter out the ones that didn't work and continue

```{r}
df_mod <- df %>% 
  filter(is.na(spotify_track_features)) %>% 
  select(-c(spotify_album_artist_ids, spotify_track_features)) %>% 
  filter(!is.na(text))

df_mod %>% 
  count(video_id, sort = T)

df_mod <- df_mod %>%
  group_by(video_id) %>% 
  mutate(across(danceability:time_signature, median)) %>% 
  ungroup() %>%
  distinct(video_id, .keep_all = T)
```

Now we have 1048 albums That's good for now.

```{r}
skimr::skim(df_mod)

set.seed(123)
spl <- initial_split(df_mod, strata = score)

train <- training(spl)
test <- testing(spl)

folds <- vfold_cv(train, 5)

```


```{r}
library(textrecipes)

basic_rec <- recipe(score ~ ., data = train) %>% 
    # update role for ids
    update_role(video_id, 
                video_title, 
                video_description, 
                score_raw,
                tags,
                url,
                video_album_query,
                spotify_artist_name,
                spotify_artist_id,
                spotify_album_name,
                spotify_album_id,
                # can come back to use release date
                spotify_album_release_date,
                track_name,
                track_id,
                track_explicit,
                new_role =  "id") %>% 
    # make score numeric ~ not sure why this has to be done again... recipes quirk
    step_mutate(score = as.numeric(score)) %>% 
    # making duration into seconds rather than 
    step_mutate(duration = as.numeric(duration)) %>% 
    # these numeric vars really need to be logged due to their distributions
    step_log(contains("statistic"), duration, spotify_artist_followers, base = 10) %>% 
    # make the genres and text tokens at the word level.
    step_tokenize(text) %>% 
    # remove stopwords
    step_stopwords(text) %>% 
    # tokenize the genres
    step_tokenize(spotify_artist_genres, token = "regex", options = list(",")) %>% 
    # we'll tune these later
    step_tokenfilter(spotify_artist_genres, max_tokens = 20) %>% 
    # we'll tune these later
    step_tokenfilter(text, max_tokens = 100) %>% 
    step_tf(text, spotify_artist_genres) %>% 
    # make date
    step_date(published_date, 
              # make lots of features just to see
              features = c("week"),
              keep_original_cols = F)


train %>% select(spotify_artist_genres)

recipe(score ~ ., data = train %>% select(score, spotify_artist_genres) %>% 
         mutate(spotify_artist_genres = str_replace_all(spotify_artist_genres, ", ", "\n"))) %>% 
  step_tokenize(spotify_artist_genres, token = "lines") %>% 
  step_tf(spotify_artist_genres) %>% 
  prep() %>% juice() %>% view()
```

### PCA and clustering

Can we cluster the different scores together? Does Fantano have a preference?

```{r}
train_clust <- train %>% select(score, danceability:time_signature)

clust_rec <- recipe(score ~ ., data = train_clust) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors())

juiced_clust <- clust_rec %>% prep() %>% juice()

juiced_clust %>% 
  ggplot(aes(PC1, PC2, colour = score)) +
  geom_point(alpha = .5) +
  scale_color_gradient2(low = "blue", high = "red", midpoint = 5)

```

If we look at each song, it seems not...

What about for the album as a whole?

```{r}
train %>% 
  count(video_id)

train_clust_group <- train %>% select(score, video_id, danceability:time_signature) %>% 
  group_by(video_id) %>% 
  summarise(across(everything(), median)) %>% 
  ungroup() %>% 
  distinct(video_id, .keep_all = T)
  
clust_rec_group <- recipe(score ~ ., data = train_clust_group) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_pca(all_numeric_predictors(), keep_original_cols = T)

juiced_clust <- clust_rec_group %>% prep() %>% juice()

juiced_clust %>% 
  ggplot(aes(PC1, PC2, colour = score)) +
  geom_point(alpha = .5) +
  scale_color_gradient2(low = "blue", high = "red", midpoint = 5)
```

What about using UMAP rather than PCA?

```{r}
library(embed)

umap_rec_group <- recipe(score ~ ., data = train_clust_group) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_umap(all_numeric_predictors())

juiced_umap <- umap_rec_group %>% prep() %>% juice()

mean_score <- juiced_umap %>% summarise(mean(score, na.rm = T)) %>% as.numeric()

juiced_umap %>% 
  mutate(mean_score = mean(score)) %>% 
  ggplot(aes(umap_1, umap_2, colour = score)) +
  geom_point(alpha = .5) +
  scale_color_gradient2(low = "blue", high = "red", midpoint = mean_score)

```


Next thing we can do is weight each of the variables, perhaps by importance in a logistic regression...

```{r}

set.seed(123)
spl <- initial_split(train_clust_group, strata = "score")

train <- training(spl)
test <- testing(spl)

fold_clust_group <- vfold_cv(train_clust_group, v = 5)

log_rec_for_var_imp <- 
  recipe(score ~ ., data = train_clust_group) %>% 
  update_role(video_id, new_role = "id") %>% 
  step_mutate(score = as.numeric(score)) %>% 
  step_normalize(all_numeric_predictors())

log_rec_for_var_imp %>% prep() %>% juice()

log_rec_for_var_imp_spec <- 
  linear_reg(penalty = tune(),
             mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") 

log_rec_for_var_imp_wf <- 
  workflow() %>% 
  add_recipe(log_rec_for_var_imp) %>% 
  add_model(log_rec_for_var_imp_spec) 

log_rec_for_var_imp_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 10)) 

log_rec_for_var_imp_tune <- 
  tune_grid(log_rec_for_var_imp_wf, resamples = fold_clust_group, grid = log_rec_for_var_imp_grid)

log_rec_for_var_imp_tune %>% 
  autoplot()

log_rec_for_var_imp_tune %>% 
  collect_metrics()

log_rec_for_var_imp_tune %>% 
  select_best(metric = "rsq")

log_rec_for_var_imp_workflow_final <- log_rec_for_var_imp_wf %>%
  finalize_workflow(parameters = select_best(log_rec_for_var_imp_tune, metric = "rsq"))
```

```{r}
log_rec_for_var_imp_fit <- log_rec_for_var_imp_workflow_final %>% 
  fit(test)

fit <- log_rec_for_var_imp_fit %>% pull_workflow_fit()

fit$fit$beta %>% 
  tidy() %>% 
  mutate(col = parse_number(column),
         abs_value = abs(value)) %>% 
  filter(col == max(col)) %>%
  slice_max(abs_value, n = 20, with_ties = F) %>% 
  mutate(row = fct_reorder(row, value)) %>% 
  ggplot(aes(value, row)) +
  geom_col() +
  geom_vline(xintercept = 0, lty = 2)

```

### Start with a logistic regression model

```{r}
library(usemodels)
usemodels::use_glmnet(formula = score ~ ., data = train)

glmnet_recipe <- 
  basic_rec %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors())

# glmnet_recipe %>% prep() %>% juice()

glmnet_spec <- 
  linear_reg(penalty = tune(),
             mixture = 1) %>% 
  set_mode("regression") %>% 
  set_engine("glmnet") 

glmnet_workflow <- 
  workflow() %>% 
  add_recipe(glmnet_recipe) %>% 
  add_model(glmnet_spec) 

# glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 10),
#                                max_tokens = c(200, 500)) 

glmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 10))

glmnet_tune <- 
  tune_grid(glmnet_workflow, resamples = folds, grid = glmnet_grid)

glmnet_tune %>% 
  autoplot()

glmnet_tune %>% 
  collect_metrics()

glmnet_tune %>% 
  select_best(metric = "rsq")

glmnet_workflow_final <- glmnet_workflow %>%
  finalize_workflow(parameters = select_best(glmnet_tune, metric = "rsq"))
```

Hard code final values for the moment

```{r}
# glmnet_workflow_final <- glmnet_workflow %>% 
#   finalize_workflow(parameters = tibble(penalty = 0.0278, mixture = 1))

glm_fit <- glmnet_workflow_final %>% 
  fit(test)

fit <- glm_fit %>% pull_workflow_fit()

fit$fit$beta %>% 
  tidy() %>% 
  mutate(col = parse_number(column),
         abs_value = abs(value)) %>% 
  filter(col == max(col)) %>%
  slice_max(abs_value, n = 50, with_ties = F) %>% 
  mutate(row = fct_reorder(row, value)) %>% 
  ggplot(aes(value, row)) +
  geom_col() +
  geom_vline(xintercept = 0, lty = 2)

```

### Now let's use a 


### Next let's go with a bagged tree model

```{r}
library(baguette)

bag_recipe <- 
  basic_rec %>% 
  step_novel(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_nzv(all_predictors()) %>% 
  step_normalize(all_numeric_predictors()) %>% 
  step_mutate(score = as.numeric(score))


bag_spec <- bag_tree() %>% 
  set_engine("rpart") %>% # 25 ensemble members 
  set_mode("regression")


bag_wf <- workflow() %>% 
    add_recipe(bag_recipe) %>% 
    add_model(bag_spec)

set.seed(123)
bag_cars <- bag_wf %>% 
    fit_resamples(resamples = folds)

bag_cars
```

